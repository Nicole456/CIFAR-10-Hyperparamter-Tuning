1
00:00:00,000 --> 00:00:01,350
So in this video,

2
00:00:01,350 --> 00:00:05,240
we're actually going to be putting together a Laplacian mechanism,

3
00:00:05,240 --> 00:00:07,380
a Laplacian randomize mechanism,

4
00:00:07,380 --> 00:00:13,335
which we are going to use to perform two different kinds of queries; sum and mean.

5
00:00:13,335 --> 00:00:16,800
We're going to see that the way in which we choose our Beta's,

6
00:00:16,800 --> 00:00:20,220
the ways in which the we formulate the amount of Laplacian noise to

7
00:00:20,220 --> 00:00:24,390
create is adjusted based on the sensitivity of sum and mean.

8
00:00:24,390 --> 00:00:26,910
Such that for mean,

9
00:00:26,910 --> 00:00:28,200
which has a smaller sensitivity,

10
00:00:28,200 --> 00:00:31,650
we end up adding a smaller amount of noise.

11
00:00:31,650 --> 00:00:33,345
So let's jump in.

12
00:00:33,345 --> 00:00:40,845
First, we're going to say our Epsilon budget for our single query is going to be 0.5,

13
00:00:40,845 --> 00:00:44,390
and now we're going to import numpy as np,

14
00:00:44,390 --> 00:00:48,830
and let's create a database, which we're going to query.

15
00:00:48,830 --> 00:00:50,675
As you will remember,

16
00:00:50,675 --> 00:00:53,460
all of these databases are the ones and zeros.

17
00:00:53,460 --> 00:00:57,200
So this is how we know that the sensitivity of addition is only one.

18
00:00:57,200 --> 00:01:02,280
So because if I were to say the sum is 54,

19
00:01:02,280 --> 00:01:04,340
but the maximum amount that this could change,

20
00:01:04,340 --> 00:01:06,705
if I remove anyone of these entries, it'll be one.

21
00:01:06,705 --> 00:01:10,649
But if this database were a database of two's,

22
00:01:10,649 --> 00:01:14,685
0-2, well then the sensitivity would actually double to two.

23
00:01:14,685 --> 00:01:17,835
Because the maximum amount of

24
00:01:17,835 --> 00:01:23,330
the output would change if I removed entry could be as high as two.

25
00:01:23,330 --> 00:01:25,635
But in particular, because we know this sort of

26
00:01:25,635 --> 00:01:28,400
apiary knowledge about a database is all zeros and ones,

27
00:01:28,400 --> 00:01:31,855
just counting the specific entries in the database,

28
00:01:31,855 --> 00:01:35,570
we can know that our sensitivity for sum is simply one.

29
00:01:35,570 --> 00:01:38,310
So let's jump into our first query,

30
00:01:39,520 --> 00:01:45,140
which is simply the sum over the database.

31
00:01:45,140 --> 00:01:51,570
So let's create our mechanism.

32
00:01:51,570 --> 00:01:56,070
So sum_query, mechanism m,

33
00:01:56,070 --> 00:01:57,510
the source of randomized mechanism.

34
00:01:57,510 --> 00:02:01,330
So what else we see? Laplacian mechanism.

35
00:02:01,330 --> 00:02:05,575
We have database and we have our query.

36
00:02:05,575 --> 00:02:07,210
So Laplacian mechanism.

37
00:02:07,210 --> 00:02:11,920
Actually, let's go ahead and put in sensitivity of a query.

38
00:02:11,920 --> 00:02:15,580
So now, the first thing we want to do is we need to

39
00:02:15,580 --> 00:02:19,060
calculate what the Beta should be for a Laplacian noise.

40
00:02:19,060 --> 00:02:25,070
So we're going to say Beta equals sensitivity divided by Epsilon.

41
00:02:25,070 --> 00:02:30,055
So as you might remember, this is how we calculate the correct Beta for Laplacian noise,

42
00:02:30,055 --> 00:02:34,125
and then we're going to say the amount of noise is equal to,

43
00:02:34,125 --> 00:02:37,350
I got to convert it to, let's see,

44
00:02:37,350 --> 00:02:38,940
what are the type was this?

45
00:02:38,940 --> 00:02:42,225
Okay, torch.tensor.

46
00:02:42,225 --> 00:02:45,090
So we're sampling values from zero to one,

47
00:02:45,090 --> 00:02:49,055
and according to this Beta spread parameter,

48
00:02:49,055 --> 00:02:52,495
this query is actually this function,

49
00:02:52,495 --> 00:02:56,895
query database, plus noise.

50
00:02:56,895 --> 00:02:59,975
This is our Laplacian mechanism.

51
00:02:59,975 --> 00:03:01,575
So let's use it, let's try it out.

52
00:03:01,575 --> 00:03:08,740
So laplacian_mechanism database some_query and sensitivity of one.

53
00:03:08,870 --> 00:03:11,530
As you can see every time we create it,

54
00:03:11,530 --> 00:03:12,850
it's a little bit different.

55
00:03:12,850 --> 00:03:18,130
The true answer is 54.

56
00:03:18,130 --> 00:03:19,855
So as you can see when we run this,

57
00:03:19,855 --> 00:03:25,840
it's just above and below 54. Pretty cool.

58
00:03:26,540 --> 00:03:31,000
I think I might have said that this was between zero and one.

59
00:03:31,000 --> 00:03:33,670
So Laplacian, it's not a range between zero and one,

60
00:03:33,670 --> 00:03:35,920
this is actually a mean centered at

61
00:03:35,920 --> 00:03:39,080
zero and then Beta is controlling the amount of spread.

62
00:03:39,380 --> 00:03:43,300
So now let's do this for a mean query.

63
00:03:43,300 --> 00:03:47,680
So def mean_query db equals db_sum,

64
00:03:47,680 --> 00:03:51,190
let's say, we just do torch.mean.

65
00:03:51,190 --> 00:03:55,290
Awesome. We have a 100 entries in our dataset.

66
00:03:55,290 --> 00:03:57,460
So the thing here is, now we know that

67
00:03:57,460 --> 00:04:02,430
the sensitivity is actually going to be one divided by 100,

68
00:04:02,430 --> 00:04:03,880
because it's the max amount that the entry could

69
00:04:03,880 --> 00:04:06,430
change divided by the total number of entries,

70
00:04:06,430 --> 00:04:10,310
because that's the output of the query if you divide it by that.

71
00:04:10,310 --> 00:04:12,505
So this sensitivity is much smaller for me.

72
00:04:12,505 --> 00:04:18,395
So if we were to use our Laplacian mechanism on the mean query,

73
00:04:18,395 --> 00:04:22,295
it's one divided by 100.

74
00:04:22,295 --> 00:04:27,470
So now, as you can see the noise is much smaller.

75
00:04:27,470 --> 00:04:31,490
The noise is just a 100th smallest,

76
00:04:31,490 --> 00:04:32,925
it's quite a bit smaller.

77
00:04:32,925 --> 00:04:34,165
Whereas if you see this,

78
00:04:34,165 --> 00:04:36,510
this one bounces around by multiple values;

79
00:04:36,510 --> 00:04:39,150
54, 53, 57, 51,

80
00:04:39,150 --> 00:04:40,910
so the noise is actually quite larger.

81
00:04:40,910 --> 00:04:42,470
Whereas, the noise here is much smaller,

82
00:04:42,470 --> 00:04:46,920
which is also appropriate because the output is also quite smaller.

83
00:04:46,920 --> 00:04:50,540
Now, the next thing to also remember is that

84
00:04:50,540 --> 00:04:54,680
this Epsilon is measuring the Epsilon that we are spending per query.

85
00:04:54,680 --> 00:04:57,410
So let's say, we had a whole budget,

86
00:04:57,410 --> 00:04:59,465
there was an Epsilon of say five,

87
00:04:59,465 --> 00:05:04,050
then we wouldn't need to partition this.

88
00:05:04,050 --> 00:05:08,790
We could do 10 queries and each with Epsilon constraint of 0.05.

89
00:05:08,790 --> 00:05:10,590
So now, let's try something else.

90
00:05:10,590 --> 00:05:15,045
So now if I say Epsilon equals 0.00001,

91
00:05:15,045 --> 00:05:16,620
a very, very, very small Epsilon.

92
00:05:16,620 --> 00:05:19,380
So almost no leakage. Watch what happens to this.

93
00:05:19,380 --> 00:05:23,910
So previously, it was dancing around the 0.54 range.

94
00:05:23,910 --> 00:05:27,650
But now, it's all over the place.

95
00:05:27,650 --> 00:05:32,630
You see how much random is that this is because we're not leaking hardly any information,

96
00:05:32,630 --> 00:05:37,280
because basically we're just returning a really big Laplacian distribution,

97
00:05:37,280 --> 00:05:41,235
and same for this Laplacian mechanism.

98
00:05:41,235 --> 00:05:44,930
So as we tutor Epsilons and make our Epsilon lower,

99
00:05:44,930 --> 00:05:48,980
we're actually increasing the amount of noise that we have to add in order to be

100
00:05:48,980 --> 00:05:53,920
able to get this really constraining level of privacy protection.

101
00:05:53,920 --> 00:05:56,500
So the other interesting thing to remember

102
00:05:56,500 --> 00:05:58,330
is that this is the same exact relationship that we

103
00:05:58,330 --> 00:06:02,515
saw when we were working with local differential privacy in the context of coin flipping.

104
00:06:02,515 --> 00:06:05,920
So the more noise that we added, the more protection,

105
00:06:05,920 --> 00:06:09,730
the more plausible deniability that we gave each participant in our survey,

106
00:06:09,730 --> 00:06:11,730
the more randomize our results we're looking.

107
00:06:11,730 --> 00:06:14,275
The more randomize our analysis ended up being,

108
00:06:14,275 --> 00:06:16,900
and the more individuals we needed to actually work

109
00:06:16,900 --> 00:06:21,215
over in order to be able to get accurate statistics.

110
00:06:21,215 --> 00:06:26,470
So similarly here, if we

111
00:06:26,470 --> 00:06:32,090
have mechanisms functions on our data-set that have smaller sensitivity,

112
00:06:32,090 --> 00:06:35,230
we can actually add less noise overall,

113
00:06:35,230 --> 00:06:39,920
simply because we're asking for a function that's just less sensitive.

114
00:06:39,920 --> 00:06:43,820
This function doesn't naturally try to leak as much information.

115
00:06:43,820 --> 00:06:48,650
So we still have this really close relationship between sensitivity,

116
00:06:48,650 --> 00:06:52,975
privacy leakage, and the number of entries in a database,

117
00:06:52,975 --> 00:06:55,550
and the accuracy of the output result.

118
00:06:55,550 --> 00:06:58,170
So it's a trade-off between these four different things,

119
00:06:58,170 --> 00:07:00,530
and that's really important understand how this trade-off

120
00:07:00,530 --> 00:07:03,655
works when we actually use this mechanism in the real world.

121
00:07:03,655 --> 00:07:06,860
So I hope you've enjoyed working with this project.

122
00:07:06,860 --> 00:07:09,020
In the next section we're going to jump into

123
00:07:09,020 --> 00:07:13,260
some more different mechanisms and ways you can use differential privacy.

