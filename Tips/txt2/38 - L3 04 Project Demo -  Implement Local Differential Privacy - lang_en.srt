1
00:00:00,000 --> 00:00:02,190
In this project, we're going to implement

2
00:00:02,190 --> 00:00:07,410
simple local differential privacy using randomized response from scratch in Python.

3
00:00:07,410 --> 00:00:10,470
Specifically, we're actually going to implement in Python,

4
00:00:10,470 --> 00:00:12,915
the coin flipping example that we just talked about.

5
00:00:12,915 --> 00:00:14,745
So as you may remember,

6
00:00:14,745 --> 00:00:18,135
if you have a group people that you're wishing to survey about a very taboo behavior,

7
00:00:18,135 --> 00:00:21,855
then we are given these interesting instructions as to how they should answer.

8
00:00:21,855 --> 00:00:25,410
So they flip a coin two times and if the first coin flip was heads,

9
00:00:25,410 --> 00:00:26,565
they should answer honestly.

10
00:00:26,565 --> 00:00:28,320
However, if he first coin flip was tails,

11
00:00:28,320 --> 00:00:30,450
they should answer according to a second coin flip.

12
00:00:30,450 --> 00:00:34,680
What this does, is this gives each person plausible deniability while simply

13
00:00:34,680 --> 00:00:40,170
averaging across all of their answers with a 50 percent probability of saying yes or no.

14
00:00:40,170 --> 00:00:43,860
Thus, if we sample across a 1000 people,

15
00:00:43,860 --> 00:00:45,810
we would carry this over 1000 people.

16
00:00:45,810 --> 00:00:48,425
We interrogate a 1000 people with this question,

17
00:00:48,425 --> 00:00:52,730
and around 60 percent of them answer, yes.

18
00:00:52,730 --> 00:00:56,570
Then we know that the true distribution is 70 percent because 70 percent,

19
00:00:56,570 --> 00:00:59,120
the true distribution has been averaged with

20
00:00:59,120 --> 00:01:02,965
50 percent to get the the output of our statistical survey.

21
00:01:02,965 --> 00:01:07,040
So we want to implement these steps in Python from scratch.

22
00:01:07,040 --> 00:01:09,500
The way that we're going to do this is like so.

23
00:01:09,500 --> 00:01:12,530
So the query function we're going to use is a mean function.

24
00:01:12,530 --> 00:01:14,060
So we're going to be taking an average.

25
00:01:14,060 --> 00:01:20,355
So let's say we have a database of size 10, or let's just say it's 100.

26
00:01:20,355 --> 00:01:23,780
So we've got a big old database of a hundred people.

27
00:01:23,780 --> 00:01:30,305
Then the true result is the mean of the database. Here we go.

28
00:01:30,305 --> 00:01:34,509
Since we generate our database randomly,

29
00:01:34,509 --> 00:01:37,600
with 50 percent probability of being one or zero,

30
00:01:37,600 --> 00:01:40,645
the mean tends to be around 0.5.

31
00:01:40,645 --> 00:01:46,650
However, we want to noise our dataset using local differential privacy.

32
00:01:46,650 --> 00:01:52,105
Local differential privacy is all about adding noise to the data itself.

33
00:01:52,105 --> 00:01:53,755
So in our case,

34
00:01:53,755 --> 00:02:01,045
adding noise to the data means replacing some of these values with random values.

35
00:02:01,045 --> 00:02:03,760
So if you remember, from this randomized response,

36
00:02:03,760 --> 00:02:05,935
like sometimes they answer honestly,

37
00:02:05,935 --> 00:02:09,340
so these would be the honest results in our example.

38
00:02:09,340 --> 00:02:14,880
Sometimes, they answer according to a coin flip, they answer randomly.

39
00:02:14,880 --> 00:02:20,420
So the first thing we need to do is actually flip two coins a 100 times.

40
00:02:20,420 --> 00:02:25,865
So we'll do first coin flip equals torch.rand,

41
00:02:25,865 --> 00:02:29,105
length of the database, greater than 0.5.

42
00:02:29,105 --> 00:02:31,460
We'll go ahead and cast this to a float as well.

43
00:02:31,460 --> 00:02:37,820
So now, we have our first coin flip for every single value in this database tensor.

44
00:02:37,820 --> 00:02:40,355
So this is the coin flip for this value,

45
00:02:40,355 --> 00:02:42,170
this is the coin flip for this value,

46
00:02:42,170 --> 00:02:43,820
this is the coin flip for this value,

47
00:02:43,820 --> 00:02:45,455
and so on and so forth.

48
00:02:45,455 --> 00:02:48,265
So now, let's do this and create a second coin flip.

49
00:02:48,265 --> 00:02:51,215
So now, this first coin flip is going to determine

50
00:02:51,215 --> 00:02:54,545
whether we want to use the value it's actually going to database,

51
00:02:54,545 --> 00:02:57,140
or whether we want to use the value that was

52
00:02:57,140 --> 00:03:00,685
randomly generated according to this second coin flipper.

53
00:03:00,685 --> 00:03:06,620
We can do this to create our noisy database or synthetic database or augmented database.

54
00:03:06,620 --> 00:03:08,690
We'll call it augmented in the following way.

55
00:03:08,690 --> 00:03:10,360
So half the time,

56
00:03:10,360 --> 00:03:12,265
so if the coin flip is one,

57
00:03:12,265 --> 00:03:14,205
then we want to use the database.

58
00:03:14,205 --> 00:03:15,825
So it was a heads answer honestly.

59
00:03:15,825 --> 00:03:17,325
So we'll call one heads.

60
00:03:17,325 --> 00:03:19,205
The nice thing here is, we can do this by simply just

61
00:03:19,205 --> 00:03:21,350
multiplying first coin flip by database,

62
00:03:21,350 --> 00:03:22,985
because this acts as a mask.

63
00:03:22,985 --> 00:03:26,210
Right. So the multiplying it times a one,

64
00:03:26,210 --> 00:03:28,610
will leave in wherever the database is and multiplying it times

65
00:03:28,610 --> 00:03:32,860
a zero will zero out any values in the database.

66
00:03:32,860 --> 00:03:37,700
So this will return database times first coin flip,

67
00:03:37,700 --> 00:03:40,340
returns a one only for

68
00:03:40,340 --> 00:03:44,030
the places in the database where there actually was a one originally.

69
00:03:44,030 --> 00:03:46,790
Go cast this to a float again.

70
00:03:46,790 --> 00:03:50,660
So here's all the ones and a few of the zeros that were

71
00:03:50,660 --> 00:03:54,100
in the original database at the first coin flip says we should keep around.

72
00:03:54,100 --> 00:03:56,700
But now, sometimes we need to lie.

73
00:03:56,700 --> 00:03:59,345
So sometimes we need to choose randomly.

74
00:03:59,345 --> 00:04:02,530
So if one minus our first coin flip,

75
00:04:02,530 --> 00:04:06,350
here's all the places where we want to actually choose randomly.

76
00:04:06,350 --> 00:04:07,870
So all of these ones that you see here,

77
00:04:07,870 --> 00:04:10,010
we want it to actually sample from the second coin flip.

78
00:04:10,010 --> 00:04:14,075
So we can do this by simply sampling or multiplying times the second coin flip.

79
00:04:14,075 --> 00:04:19,260
So here's all the values that are being sampled randomly.

80
00:04:19,260 --> 00:04:21,505
So now, if we simply add these together,

81
00:04:21,505 --> 00:04:27,530
then we get a new augmented database, which is differentially private.

82
00:04:27,530 --> 00:04:29,705
Cool. So far so good.

83
00:04:29,705 --> 00:04:36,245
So again, local differential privacy is all about adding noise to the database entries.

84
00:04:36,245 --> 00:04:38,510
So that any queries that we do in

85
00:04:38,510 --> 00:04:40,970
this database have a certain amount of differential privacy.

86
00:04:40,970 --> 00:04:43,805
However, in doing so,

87
00:04:43,805 --> 00:04:47,105
we have done something statistically significant.

88
00:04:47,105 --> 00:04:51,950
So we might try to say we can just query the database and be done.

89
00:04:51,950 --> 00:04:57,120
So torch.mean database.float and boom,

90
00:04:57,120 --> 00:05:01,025
so we augment the database and we can just go ahead and use this.

91
00:05:01,025 --> 00:05:04,250
However, something else has happened here.

92
00:05:04,250 --> 00:05:09,140
So half of our values are honest and half of

93
00:05:09,140 --> 00:05:15,410
our values are always going to have a mean or try to have a mean, that's at 0.5.

94
00:05:15,410 --> 00:05:22,060
This means that it's going to skew the output of our query towards 0.5.

95
00:05:22,060 --> 00:05:23,650
Because half the time,

96
00:05:23,650 --> 00:05:26,960
we're using the values that are from the second coin flip,

97
00:05:26,960 --> 00:05:30,455
which has a 50/50 chance of being positive or being heads or tails.

98
00:05:30,455 --> 00:05:33,730
So let's just say the database had,

99
00:05:33,730 --> 00:05:36,750
on average was 70 percent of the time that

100
00:05:36,750 --> 00:05:40,145
people said true for whatever the taboo behavior was.

101
00:05:40,145 --> 00:05:46,720
So this database is actual values reflecting from people.

102
00:05:46,720 --> 00:05:51,180
Let's say that 70 percent of these were true,

103
00:05:51,180 --> 00:05:53,965
so that the mean of this would be 0.7.

104
00:05:53,965 --> 00:05:57,610
That would mean that if we took half the values from here and half the values

105
00:05:57,610 --> 00:06:01,435
from this randomly generated 50/50 distribution,

106
00:06:01,435 --> 00:06:04,090
then the mean of them would shift from this where

107
00:06:04,090 --> 00:06:07,404
the mean was 0.7 and this where the mean was 0.5,

108
00:06:07,404 --> 00:06:09,295
would be halves between them.

109
00:06:09,295 --> 00:06:14,530
The mean would then be 0.6 even though the original distribution was 0.7.

110
00:06:14,530 --> 00:06:18,730
Right. So it's almost like this query is now skewed as a result of the noise.

111
00:06:18,730 --> 00:06:20,710
Skewing was not all we were after,

112
00:06:20,710 --> 00:06:24,550
what we wanted to do was just give each individual person plausible deniability.

113
00:06:24,550 --> 00:06:28,110
So we have to do one final step here, which is deskew the result.

114
00:06:28,110 --> 00:06:30,000
So the interesting thing is here,

115
00:06:30,000 --> 00:06:32,040
if we know that,

116
00:06:32,040 --> 00:06:34,560
say for example, there's a distribution

117
00:06:34,560 --> 00:06:38,240
here where there's a certain averages here and we'll say it's 70 percent.

118
00:06:38,240 --> 00:06:41,030
The average of this is always going to be 50 percent.

119
00:06:41,030 --> 00:06:43,520
So that we know that the average between these two datasets.

120
00:06:43,520 --> 00:06:47,710
If we are half from here and half from here is 60 percent,

121
00:06:47,710 --> 00:06:50,040
then we can just do that in reverse.

122
00:06:50,040 --> 00:06:51,950
So that means that if the output of

123
00:06:51,950 --> 00:06:55,550
our augmented database gives us a mean of 60 percent, well,

124
00:06:55,550 --> 00:06:59,600
then, we know that it's really trying to tell us 70 percent

125
00:06:59,600 --> 00:07:04,230
because half of our data is just 50/50 random noise.

126
00:07:04,230 --> 00:07:08,615
So I can actually go and try to deskew this result.

127
00:07:08,615 --> 00:07:12,715
This is the true output of our database.

128
00:07:12,715 --> 00:07:17,090
Because I have removed the skewed average that was

129
00:07:17,090 --> 00:07:22,460
a result of this local differential privacy or this second coin flips.

130
00:07:22,460 --> 00:07:26,585
So this is actually the output of our augmented result,

131
00:07:26,585 --> 00:07:32,970
so our augmented differentially private result. We'll call it db result.

132
00:07:32,970 --> 00:07:39,560
So now, we can package all this as a single query and see what was in our assignments.

133
00:07:39,560 --> 00:07:43,760
So in our assignment, we needed to return the augmented result,

134
00:07:43,760 --> 00:07:45,380
that differentially private result.

135
00:07:45,380 --> 00:07:47,540
We also want to return the true result,

136
00:07:47,540 --> 00:07:50,510
because the notion of this and what we wanted to learn from this lesson is

137
00:07:50,510 --> 00:07:53,990
actually to understand how these results tend to change.

138
00:07:53,990 --> 00:07:57,290
So let's go ahead and return the true result or

139
00:07:57,290 --> 00:08:01,550
compute the true result as well. Here we go.

140
00:08:01,550 --> 00:08:04,625
Okay. So now, what we want to do here,

141
00:08:04,625 --> 00:08:08,240
is work on databases at different sizes.

142
00:08:08,240 --> 00:08:13,220
So I believe in the assignment we were supposed to do a database of size 10.

143
00:08:13,220 --> 00:08:19,970
Then look at the private result, true results.

144
00:08:19,970 --> 00:08:21,800
We'll print this out here so it's easy to see.

145
00:08:21,800 --> 00:08:24,830
So with noise and without noise.

146
00:08:24,830 --> 00:08:26,300
This is the true answer.

147
00:08:26,300 --> 00:08:30,700
What we're going to see when we compare these, is that, man,

148
00:08:30,700 --> 00:08:35,790
that noise really throws things off sometimes but let's use a bigger dataset.

149
00:08:35,790 --> 00:08:37,610
Okay. That's quite a bit closer.

150
00:08:37,610 --> 00:08:39,880
Let's use an even bigger dataset.

151
00:08:39,880 --> 00:08:42,220
Now, we're getting really close.

152
00:08:42,220 --> 00:08:44,380
Then, even bigger dataset.

153
00:08:44,380 --> 00:08:48,755
So we'll put 10,000 entries and that is getting really close.

154
00:08:48,755 --> 00:08:50,550
So here's the thing to remember,

155
00:08:50,550 --> 00:08:52,150
about local differential privacy and

156
00:08:52,150 --> 00:08:53,960
really actually about differential privacy in general,

157
00:08:53,960 --> 00:08:58,345
whenever we're adding noise to a distribution, were corrupting it.

158
00:08:58,345 --> 00:09:01,280
So the statistics that the queries that

159
00:09:01,280 --> 00:09:03,845
we're doing are going to be sensitive to this noise.

160
00:09:03,845 --> 00:09:08,525
However, the more data points that we have,

161
00:09:08,525 --> 00:09:12,170
the more this noise will tend to average out.

162
00:09:12,170 --> 00:09:15,680
It will tend to not affect the output of the query because on average,

163
00:09:15,680 --> 00:09:17,600
across a large number of people,

164
00:09:17,600 --> 00:09:19,790
sometimes the noise is making the result

165
00:09:19,790 --> 00:09:22,265
higher than it should be or lower than the result should be.

166
00:09:22,265 --> 00:09:25,460
But on average, it's actually still centered

167
00:09:25,460 --> 00:09:28,755
around the same mean of the true data distribution.

168
00:09:28,755 --> 00:09:31,400
In particular, a general rule of thumb, which

169
00:09:31,400 --> 00:09:34,280
is local differential privacy is very data hungry.

170
00:09:34,280 --> 00:09:37,190
In order to be able to noise the dataset, you're adding a ton of noise,

171
00:09:37,190 --> 00:09:39,860
we're adding noise to every single value in the dataset.

172
00:09:39,860 --> 00:09:41,960
So when we have 10,000 entries,

173
00:09:41,960 --> 00:09:44,670
we're adding 10,000 bits of noise.

174
00:09:44,670 --> 00:09:46,400
I guess like with 50 percent probability.

175
00:09:46,400 --> 00:09:47,990
So it probably more like 5,000 bits of noise,

176
00:09:47,990 --> 00:09:49,370
but that's still a lot of noise.

177
00:09:49,370 --> 00:09:51,950
So global differential privacy, in contrast,

178
00:09:51,950 --> 00:09:54,785
only adds noise to the output of the query.

179
00:09:54,785 --> 00:09:56,840
We will find that this tends to be

180
00:09:56,840 --> 00:10:00,740
quite a bit more accurate and a little less data hungry than local differential privacy.

181
00:10:00,740 --> 00:10:02,060
So if you want to implement

182
00:10:02,060 --> 00:10:05,300
local differential privacy and protect data at the data level,

183
00:10:05,300 --> 00:10:06,950
protect the dataset itself.

184
00:10:06,950 --> 00:10:09,680
Then, you'll want to use local differential privacy,

185
00:10:09,680 --> 00:10:11,960
that we want to make sure you have a really large dataset.

186
00:10:11,960 --> 00:10:13,880
Whereas, if you want to use

187
00:10:13,880 --> 00:10:15,980
a less data hungry algorithms and

188
00:10:15,980 --> 00:10:18,320
maybe your dataset is smaller but you still need to protect it,

189
00:10:18,320 --> 00:10:21,170
then it's better to lean towards global differential privacy.

190
00:10:21,170 --> 00:10:25,730
In general, I think personally leaning towards global differential privacy seems

191
00:10:25,730 --> 00:10:27,980
to be where a lot of the field is going

192
00:10:27,980 --> 00:10:30,520
although obviously the local db has its advantages.

193
00:10:30,520 --> 00:10:34,520
So the event the local db is that in theory you could publish some of these datasets.

194
00:10:34,520 --> 00:10:36,230
So you don't have to trust someone

195
00:10:36,230 --> 00:10:39,950
to lower trust settings, but we'll get into those later.

196
00:10:39,950 --> 00:10:42,455
But for now, just understand this notion of

197
00:10:42,455 --> 00:10:46,260
this trade off between small datasets and and large datasets.

198
00:10:46,260 --> 00:10:51,005
Meaning that your noise creates vastly different outputs of the queries,

199
00:10:51,005 --> 00:10:54,860
versus only slightly different outputs of the queries

200
00:10:54,860 --> 00:10:59,260
when you have a large enough dataset to average the noise over.

