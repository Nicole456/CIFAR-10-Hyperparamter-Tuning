1
00:00:00,000 --> 00:00:01,830
In this section, we're going to learn how to view

2
00:00:01,830 --> 00:00:04,080
federated learning with a trusted aggregator,

3
00:00:04,080 --> 00:00:07,515
which means that when we actually aggregate our gradients from multiple workers,

4
00:00:07,515 --> 00:00:10,455
instead of bringing them to a central server to aggregate them,

5
00:00:10,455 --> 00:00:13,770
or having them send gradients directly to each other.

6
00:00:13,770 --> 00:00:17,400
We're going to have everyone who's involved in the federated learning process or

7
00:00:17,400 --> 00:00:23,160
the federated learning session send all of their gradients to a neutral third party,

8
00:00:23,160 --> 00:00:26,175
which we're going to call a secure worker.

9
00:00:26,175 --> 00:00:30,465
So first, let's set up PySyft.

10
00:00:30,465 --> 00:00:33,510
So we're going to do all the normal things we did before.

11
00:00:33,510 --> 00:00:36,360
So, import syft, import torch,

12
00:00:36,360 --> 00:00:39,910
hook torch, and then import a few other things from torch.

13
00:00:40,550 --> 00:00:44,610
Then let's create a few workers.

14
00:00:44,610 --> 00:00:47,160
So in this case, we're going to create three workers;

15
00:00:47,160 --> 00:00:48,980
Bob, Alice, and secure worker.

16
00:00:48,980 --> 00:00:51,280
So Bob and Alice are going to be our data owners,

17
00:00:51,280 --> 00:00:54,515
and secure worker is going to be this trusted third party

18
00:00:54,515 --> 00:00:59,370
that is able to perform that good aggregation.

19
00:00:59,370 --> 00:01:02,270
Now, we're doing this additional step which

20
00:01:02,270 --> 00:01:05,300
for virtual workers is not strictly speaking necessary,

21
00:01:05,300 --> 00:01:07,640
however, for workers in a real-world,

22
00:01:07,640 --> 00:01:09,170
if you're going to use socket workers and some of

23
00:01:09,170 --> 00:01:12,065
these other HTTP worker is you actually do need to do this.

24
00:01:12,065 --> 00:01:14,300
You do inform workers that other workers exist.

25
00:01:14,300 --> 00:01:16,240
So we have Bob, Alice and secure worker,

26
00:01:16,240 --> 00:01:20,390
we're just letting them know that the address is.

27
00:01:20,390 --> 00:01:23,165
We have a reference to these other workers.

28
00:01:23,165 --> 00:01:27,910
So now we're going to have use the same toy data set that we used in the last example,

29
00:01:27,910 --> 00:01:30,580
and we're going to train the same simple linear model

30
00:01:30,580 --> 00:01:32,860
just to keep everything pretty simple.

31
00:01:32,860 --> 00:01:35,320
So that's our data and target.

32
00:01:35,320 --> 00:01:38,410
Here's Bob's data, so we'll set it to Bob.

33
00:01:38,410 --> 00:01:41,815
Here's Alice's data, which we'll send to Alice.

34
00:01:41,815 --> 00:01:47,000
Then we'll initialize a toy model to be the same model that we were working with before.

35
00:01:47,000 --> 00:01:48,610
So now, we're all set up.

36
00:01:48,610 --> 00:01:50,515
So we've got our model and our central server.

37
00:01:50,515 --> 00:01:52,674
You've got a data set that's distributed,

38
00:01:52,674 --> 00:01:56,155
and now let's start working on federated learning.

39
00:01:56,155 --> 00:02:00,090
So the first thing we're going to do it's a little bit different this time.

40
00:02:00,090 --> 00:02:04,120
Is that instead of just having one model that we

41
00:02:04,120 --> 00:02:09,850
send to and from each individual person in synchrony.

42
00:02:09,850 --> 00:02:12,200
We actually want to have two different models that we

43
00:02:12,200 --> 00:02:14,630
send to each of the workers because we want that to be averaged.

44
00:02:14,630 --> 00:02:23,220
So we have Bob's model which equals model.copy and we're going to send it to Bob.

45
00:02:23,360 --> 00:02:29,270
We're going to have Alice's model, which is a copy of the original model,

46
00:02:29,270 --> 00:02:32,190
and we're going to send it to Alice.

47
00:02:32,190 --> 00:02:36,650
This also means that we need to have two separate optimizers.

48
00:02:36,650 --> 00:02:40,780
So we'd have one for the parameters Bob's model and one for the parameters of Alice's.

49
00:02:40,780 --> 00:02:44,125
So Bob's optimizer equals optim.

50
00:02:44,125 --> 00:02:46,375
So this is just standard by torch stuff.

51
00:02:46,375 --> 00:02:55,035
Sgd params equals bobs_model.parameters, parameter equals 0.1.

52
00:02:55,035 --> 00:03:00,070
Then Alice's optimizer equals optim.

53
00:03:00,070 --> 00:03:09,500
SG descent params Alice's_ model.parameters, parameter equals 0.01.

54
00:03:09,500 --> 00:03:16,880
Great. Now, let's do a step of training for Bob's model.

55
00:03:16,880 --> 00:03:21,040
So first thing we'll do is bobs_opt.zero_grad,

56
00:03:21,040 --> 00:03:23,085
so zero out the gradients.

57
00:03:23,085 --> 00:03:29,860
bobs_pred is just going to be bobs_model, bobs_data.

58
00:03:32,530 --> 00:03:39,270
As you can see, this is all working with the pointers as it should, bobs_loss.

59
00:03:45,410 --> 00:03:48,730
Then we're back propagating.

60
00:03:53,620 --> 00:04:01,365
Beautiful. Now, we want to do bobs_opt.step.

61
00:04:01,365 --> 00:04:07,575
So the weight update and then we'll just get bobs_loss right now.

62
00:04:07,575 --> 00:04:20,100
So my O key

63
00:04:20,100 --> 00:04:25,245
is not doing what it's supposed to.

64
00:04:25,245 --> 00:04:34,870
That is super odd. Okay. Well, I'm going to just copy that for now.

65
00:04:35,990 --> 00:04:37,395
All right.

66
00:04:37,395 --> 00:04:38,860
Okay.

67
00:04:52,580 --> 00:04:54,090
There we go.

68
00:04:54,090 --> 00:04:55,920
If we do this multiple times,

69
00:04:55,920 --> 00:04:58,885
we can see that Bob's loss goes down.

70
00:04:58,885 --> 00:05:03,790
So now I also want to do this separately for Alice.

71
00:05:04,230 --> 00:05:11,110
We can just replace all references to Bob to the references to Alice.

72
00:05:17,140 --> 00:05:20,520
Make sure I don't miss any here.

73
00:05:49,970 --> 00:05:55,470
Like it might have been plural.

74
00:06:00,490 --> 00:06:04,805
All right. It seems to learn. So now

75
00:06:04,805 --> 00:06:08,580
we've got this ability to train Bob's analysis workers.

76
00:06:19,300 --> 00:06:21,500
But then, we come to the question

77
00:06:21,500 --> 00:06:25,835
of how are we going to average these two models together.

78
00:06:25,835 --> 00:06:28,700
So we have Bob's model, we have Alice's model.

79
00:06:28,700 --> 00:06:31,865
We trained both of them separately on these different machines.

80
00:06:31,865 --> 00:06:33,680
But now I want to be able to average them together.

81
00:06:33,680 --> 00:06:40,170
Well, we have this nice convenience method or we can go alices _model.move secure_worker.

82
00:06:40,170 --> 00:06:42,000
What this does and it's similar

83
00:06:42,000 --> 00:06:45,440
to.send that you might have used previously on the worker.

84
00:06:45,440 --> 00:06:46,610
So it just iterates through

85
00:06:46,610 --> 00:06:51,125
every parameter analysis model and calls.move on that parameter inline.

86
00:06:51,125 --> 00:06:56,320
So now, we can do bobs_model.moves secure_worker.

87
00:06:58,970 --> 00:07:04,980
Now, both Bob and Alice's model are on secure_workers.

88
00:07:04,980 --> 00:07:07,460
So if we look at secure_worker._objects we can

89
00:07:07,460 --> 00:07:11,220
see here all the parameters for Bob's analysis model.

90
00:07:13,490 --> 00:07:17,570
Now, all we need to do is just average them together.

91
00:07:17,570 --> 00:07:26,190
So if you have alices_model.weight.data plus bobs_model.weight.data.

92
00:07:28,780 --> 00:07:32,890
I'm going to average them, so we'll divide them by two.

93
00:07:32,990 --> 00:07:35,880
Then we'll call it.get.

94
00:07:35,880 --> 00:07:37,650
then we'll say, model,

95
00:07:37,650 --> 00:07:45,450
which is our global model,.weight.data.set to read this, boom.

96
00:07:45,450 --> 00:07:48,540
Now, we just averaged our weights,

97
00:07:48,540 --> 00:07:51,305
and then we'll do the same thing for bias.

98
00:07:51,305 --> 00:07:56,160
So these are the two parameters that we have in linear model.

99
00:07:57,800 --> 00:08:01,910
Now, we have just average our models and brought them back up,

100
00:08:01,910 --> 00:08:06,275
brought just the average version of the model backup to the central secure server.

101
00:08:06,275 --> 00:08:09,170
So now all we really need to do is just put this whole thing in a

102
00:08:09,170 --> 00:08:12,175
big for loop so that we can rinse and repeat.

103
00:08:12,175 --> 00:08:17,160
So let's see here.

104
00:08:19,130 --> 00:08:23,745
We can take this, and this,

105
00:08:23,745 --> 00:08:33,670
and this, and then we move it. We average it.

106
00:08:44,300 --> 00:08:53,235
So we'll call this for num_rounds in, for round_iter.

107
00:08:53,235 --> 00:09:00,030
So at one session of this is called a round. So that could enable iter.

108
00:09:00,580 --> 00:09:04,180
This is the number of iterations in the round.

109
00:09:04,180 --> 00:09:11,200
We can actually print Bob and Alice's last loss.

110
00:09:23,900 --> 00:09:28,430
There you have it. Federated learning where

111
00:09:28,430 --> 00:09:34,340
the gradient aggregation happens on a neutral third party.

112
00:09:34,340 --> 00:09:39,110
So it just makes it so that each sort of layer of additional functionality adds

113
00:09:39,110 --> 00:09:41,990
a little bit more privacy and a little more flexibility

114
00:09:41,990 --> 00:09:45,540
to the different scenarios that we can use federated learning in.

115
00:09:45,540 --> 00:09:48,320
So in this case, now we are flexible enough to be able to

116
00:09:48,320 --> 00:09:51,290
do federated learning where we work with a secure worker,

117
00:09:51,290 --> 00:09:53,045
a third third-party aggregator.

118
00:09:53,045 --> 00:09:56,720
Actually this kind of setup is actually the most popular one.

119
00:09:56,720 --> 00:09:59,330
So in several large-scale deployments,

120
00:09:59,330 --> 00:10:06,435
actually use a trusted machine that is trusted to aggregate weights together,

121
00:10:06,435 --> 00:10:09,150
and then delete what it has leftover.

122
00:10:09,150 --> 00:10:10,500
We didn't put the delete part in here,

123
00:10:10,500 --> 00:10:17,550
but we could certainly do that if we did secure_worker.clear_objects. There you go.

124
00:10:17,550 --> 00:10:22,320
Now as your worker reliably got rid

125
00:10:22,320 --> 00:10:27,320
of all of the tensors that have had stored on this machine.

126
00:10:27,320 --> 00:10:30,095
So there you go. I hope you've enjoyed this project.

127
00:10:30,095 --> 00:10:34,320
In the next section, we're going to build on this a little bit more

