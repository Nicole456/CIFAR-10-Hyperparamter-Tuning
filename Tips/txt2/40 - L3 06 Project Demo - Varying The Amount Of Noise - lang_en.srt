1
00:00:00,000 --> 00:00:02,610
So in this project, we're going to take the code that we wrote in

2
00:00:02,610 --> 00:00:05,730
the last section and we're going to add one extra feature.

3
00:00:05,730 --> 00:00:08,610
That extra feature is that we're going to make it so that we can bias

4
00:00:08,610 --> 00:00:14,385
the first coin flip to be arbitrarily more likely or less likely to be heads.

5
00:00:14,385 --> 00:00:17,850
The thing that we're going to need to change is how we actually

6
00:00:17,850 --> 00:00:22,395
reskew the output of our mean query later.

7
00:00:22,395 --> 00:00:25,545
So the first thing we're going to do is go ahead and copy this query.

8
00:00:25,545 --> 00:00:27,780
So this is the code that we had from last time.

9
00:00:27,780 --> 00:00:31,275
We have this first coin flip and we determine

10
00:00:31,275 --> 00:00:35,505
the likelihood of this coin flip to be a heads or tails based on this threshold.

11
00:00:35,505 --> 00:00:37,800
So the torch code here actually,

12
00:00:37,800 --> 00:00:41,055
these outputs, a random distribution of numbers between zero and one.

13
00:00:41,055 --> 00:00:43,035
So if we look at here,

14
00:00:43,035 --> 00:00:45,060
it's a bunch of random numbers,

15
00:00:45,060 --> 00:00:47,655
just uniformly random between zero and one.

16
00:00:47,655 --> 00:00:51,660
We threshold it so that say 50 percent of them

17
00:00:51,660 --> 00:00:55,710
on average are above the threshold and 50 percent are below.

18
00:00:55,710 --> 00:00:59,790
So now, what we want to do in this section is we want to make this adjustable.

19
00:00:59,790 --> 00:01:03,830
We want to make this a noise parameter which actually

20
00:01:03,830 --> 00:01:08,885
sets the likelihood that the coin flip will be a heads.

21
00:01:08,885 --> 00:01:13,475
So let's say 0.2, maybe 0.2 percent probability of being a heads.

22
00:01:13,475 --> 00:01:15,890
So how does these change things?

23
00:01:15,890 --> 00:01:18,755
The interesting thing here is all the rest of the pieces are really the same.

24
00:01:18,755 --> 00:01:22,160
So the first coin flip, we've modified with this a little bit.

25
00:01:22,160 --> 00:01:27,185
The second coin flip is still a random distribution between a 50/50 coin flip,

26
00:01:27,185 --> 00:01:31,400
just sometimes we're more or less likely to actually rely on

27
00:01:31,400 --> 00:01:33,440
the second coin flip depending on

28
00:01:33,440 --> 00:01:36,725
the likelihood that our first coin flip is a heads or tails.

29
00:01:36,725 --> 00:01:39,170
Augmented database is still created in the same way

30
00:01:39,170 --> 00:01:41,660
because it's still based on these two coin flips.

31
00:01:41,660 --> 00:01:48,265
The part that's different however is how we reskew the results for this mean query later.

32
00:01:48,265 --> 00:01:52,250
Because the nature of this average is based on

33
00:01:52,250 --> 00:01:58,140
the idea that we are reskewing the data set to have the same average,

34
00:01:58,140 --> 00:02:01,050
to have the same mean as the original database.

35
00:02:01,050 --> 00:02:03,560
We take whatever the actual database was.

36
00:02:03,560 --> 00:02:05,000
So in one example earlier,

37
00:02:05,000 --> 00:02:07,730
we mentioned that perhaps 70 percent of people on

38
00:02:07,730 --> 00:02:11,065
average answered true to whatever the question was.

39
00:02:11,065 --> 00:02:13,940
We take whatever this distribution is so that we'll say it's centered around

40
00:02:13,940 --> 00:02:18,590
0.7 and we average it with a distribution that is centered around 0.5,

41
00:02:18,590 --> 00:02:22,840
then that returns a distribution if they're weighted evenly that's averaged around 0.6.

42
00:02:22,840 --> 00:02:26,670
So we had a true distribution mean,

43
00:02:26,670 --> 00:02:28,245
let's just say it was 0.7.

44
00:02:28,245 --> 00:02:31,740
Then we say that our noise distribution,

45
00:02:31,740 --> 00:02:34,200
we'll say it's 0.5 because it's a random coin flip.

46
00:02:34,200 --> 00:02:38,460
A 70 percent of people said yes to our question,

47
00:02:38,460 --> 00:02:42,195
and then 50/50 coin flip.

48
00:02:42,195 --> 00:02:46,960
So this means that the augmented database mean is going to

49
00:02:46,960 --> 00:02:52,505
be "true_dist mean times noise_dist_ mean divided by two."

50
00:02:52,505 --> 00:02:58,490
Oh, plus, sorry. However, we want to get rid of this random noise

51
00:02:58,490 --> 00:03:04,760
on average while still keeping the noise that we actually put on each individual entry.

52
00:03:04,760 --> 00:03:08,930
More importantly, if we are more or less likely to choose as distribution,

53
00:03:08,930 --> 00:03:14,000
we want to make sure that however we reskew this so that the means are correct,

54
00:03:14,000 --> 00:03:17,905
we can properly deskew it according to this noise parameter.

55
00:03:17,905 --> 00:03:22,085
What we really do here is we actually sort of run this mean in reverse.

56
00:03:22,085 --> 00:03:26,060
We're unaveraging the output of the query.

57
00:03:26,060 --> 00:03:29,930
So we'll go ahead and pop this out so we can work with it out here.

58
00:03:29,930 --> 00:03:32,125
So this is the one we're going to work with.

59
00:03:32,125 --> 00:03:35,415
First, we're going to create a sort of in which it's actually our skewed query.

60
00:03:35,415 --> 00:03:37,040
So the output of our query.

61
00:03:37,040 --> 00:03:39,050
So our skewed result,

62
00:03:39,050 --> 00:03:42,680
which equals augmented database dot float dot mean.

63
00:03:42,680 --> 00:03:45,740
So it's a skewed result. So this is the wrong result.

64
00:03:45,740 --> 00:03:50,535
We need to basically unaverage this with 0.5.

65
00:03:50,535 --> 00:03:53,010
It's being averaged at 0.5 but it's a weighted average and

66
00:03:53,010 --> 00:03:55,595
that weighted average according to this weight.

67
00:03:55,595 --> 00:03:58,910
Yeah, let's talk about this analogy down here just a little bit longer.

68
00:03:58,910 --> 00:04:01,825
So if we say our noise parameter is 0.2,

69
00:04:01,825 --> 00:04:03,225
in our original example,

70
00:04:03,225 --> 00:04:06,170
this was 50/50 first coin flip,

71
00:04:06,170 --> 00:04:07,910
which meant that 50 percent of the time,

72
00:04:07,910 --> 00:04:10,325
we use the true distribution with a mean of 0.7,

73
00:04:10,325 --> 00:04:12,120
and 50 percent of the time,

74
00:04:12,120 --> 00:04:15,020
we use a distribution that also had 0.5.

75
00:04:15,020 --> 00:04:16,880
So another way of thinking about this is that

76
00:04:16,880 --> 00:04:21,590
our true distribution mean was being multiplied by noise.

77
00:04:21,590 --> 00:04:24,215
So half of the time, we're using this distribution,

78
00:04:24,215 --> 00:04:26,410
and the other half of the time,

79
00:04:26,410 --> 00:04:31,130
we were using noise distribution mean one minus noise.

80
00:04:31,130 --> 00:04:34,600
So how do we then reverse this?

81
00:04:34,600 --> 00:04:39,845
Well, basically we can do simple algebra and pull out

82
00:04:39,845 --> 00:04:45,950
what the true distribution mean was by removing all these other terms.

83
00:04:45,950 --> 00:04:48,560
For de-using all these different terms.

84
00:04:48,560 --> 00:04:53,150
It's going to be a multiplications attraction component because we want to get

85
00:04:53,150 --> 00:04:58,390
out this value or at least deskew this value according to these others.

86
00:04:58,390 --> 00:05:00,200
So let's go ahead and do that.

87
00:05:00,200 --> 00:05:02,345
So the way in which we do this is we say,

88
00:05:02,345 --> 00:05:03,815
we take our skewed result,

89
00:05:03,815 --> 00:05:08,480
and we say our final or augments result or private result is going

90
00:05:08,480 --> 00:05:13,280
to equal our skewed result divided by our noise minus 0.5,

91
00:05:13,280 --> 00:05:16,490
and the 0.5 is this 0.5 right here.

92
00:05:16,490 --> 00:05:21,935
Then, we're going to multiply this times noise divided by one minus noise.

93
00:05:21,935 --> 00:05:23,840
The deal that's going on right here is what

94
00:05:23,840 --> 00:05:25,760
we're basically deskewing these results so that

95
00:05:25,760 --> 00:05:30,170
our private result has its mean adjusted according to this noise parameter.

96
00:05:30,170 --> 00:05:32,040
So this noise parameter was at 0.5,

97
00:05:32,040 --> 00:05:33,565
then the skewed result,

98
00:05:33,565 --> 00:05:37,790
the distance between a skewed result and the private result is basically

99
00:05:37,790 --> 00:05:42,320
this is halfway between the true mean of distribution,

100
00:05:42,320 --> 00:05:45,965
and the mean of the distribution of 50/50 coin flip.

101
00:05:45,965 --> 00:05:47,915
So notice that 0.5.

102
00:05:47,915 --> 00:05:55,800
So 0.5 minus 0.4929, 0.5 minus 0.4858.

103
00:05:55,940 --> 00:05:58,500
This is roughly half of this,

104
00:05:58,500 --> 00:06:02,690
so we're doubling the distance because there was a 50/50 average.

105
00:06:02,690 --> 00:06:08,600
So as we can see, this is the correct algebraic deskewing formula.

106
00:06:08,600 --> 00:06:11,095
Then we return to the same things we did before.

107
00:06:11,095 --> 00:06:15,565
Okay. Now, we want to do a similar experiment to what we did last time.

108
00:06:15,565 --> 00:06:19,405
However, instead of varying the size of the database,

109
00:06:19,405 --> 00:06:23,030
we want to vary the amount of noise that we're adding to the database.

110
00:06:23,030 --> 00:06:25,580
So we're going to keep the database, I think it was the size of 100.

111
00:06:25,580 --> 00:06:29,115
Then, we're going to have this noise parameter.

112
00:06:29,115 --> 00:06:31,995
Let's start at like 0.1,

113
00:06:31,995 --> 00:06:35,195
it's relatively low noise 0.2,

114
00:06:35,195 --> 00:06:38,670
and we're just going to keep doubling and see what happens.

115
00:06:38,860 --> 00:06:42,855
It's changing less than I expected. Oh, we do not run this.

116
00:06:42,855 --> 00:06:45,480
There we go. It's changing quite a bit less than expected,

117
00:06:45,480 --> 00:06:47,835
actually. Let's just keep going.

118
00:06:47,835 --> 00:06:53,765
Typo. Results. As we increase the amount of noise,

119
00:06:53,765 --> 00:06:58,820
the difference between on average starts getting quite a bit more.

120
00:06:58,820 --> 00:07:02,855
But if we counter this with a large dataset,

121
00:07:02,855 --> 00:07:04,580
then they come back together.

122
00:07:04,580 --> 00:07:08,870
So the size of the data set allows you to add more noise or

123
00:07:08,870 --> 00:07:10,820
more privacy protection to

124
00:07:10,820 --> 00:07:14,210
the individuals who are inside the dataset. This is an interesting trade-off.

125
00:07:14,210 --> 00:07:19,880
The counter-intuitive thing here is that the more private data you have access to,

126
00:07:19,880 --> 00:07:23,980
the easier it is to protect the privacy of the people who were involved.

127
00:07:23,980 --> 00:07:28,520
So the larger this corpus of dataset is,

128
00:07:28,520 --> 00:07:33,830
the more noise you can add while still getting an accurate result.

129
00:07:33,830 --> 00:07:38,415
Now, in society, this is actually probably even more counter-intuitive

130
00:07:38,415 --> 00:07:41,000
because people think of preserving privacy is

131
00:07:41,000 --> 00:07:43,850
as giving statistician's access to less and less data,

132
00:07:43,850 --> 00:07:46,805
but in fact, with differential privacy,

133
00:07:46,805 --> 00:07:48,320
the opposite is true.

134
00:07:48,320 --> 00:07:51,980
Because the intuition behind differential privacy is about

135
00:07:51,980 --> 00:07:55,620
saying we want to learn about an aggregation over a large corpus.

136
00:07:55,620 --> 00:07:59,400
We want to learn something that is common about many different people.

137
00:07:59,400 --> 00:08:00,800
So one thing might be,

138
00:08:00,800 --> 00:08:04,110
let's say you're performing statistical analysis of medical records.

139
00:08:04,110 --> 00:08:08,450
Let's say you're going to identify tumors inside of MRI scans.

140
00:08:08,450 --> 00:08:12,230
You have a collection of say 1,000 images that have

141
00:08:12,230 --> 00:08:14,210
the tumors annotated so you're trying to learn how to

142
00:08:14,210 --> 00:08:16,460
detect tumors inside of individuals.

143
00:08:16,460 --> 00:08:18,170
When you're performing a statistical analysis,

144
00:08:18,170 --> 00:08:22,630
you're not actually interested in whether any one of these people has a tumor.

145
00:08:22,630 --> 00:08:25,640
Now, you're not trying to violate any particular person's privacy, instead,

146
00:08:25,640 --> 00:08:31,210
you're trying to do a statistical study to understand what do tumors in humans look like.

147
00:08:31,210 --> 00:08:35,630
You're actually trying to go after information that is fundamentally public but just

148
00:08:35,630 --> 00:08:40,160
happens to be buried inside of individual private data points.

149
00:08:40,160 --> 00:08:42,650
More importantly, the way in which

150
00:08:42,650 --> 00:08:49,750
this technology works is that the differential privacy is a very complex kind of

151
00:08:49,750 --> 00:08:54,160
filter and the way that the differentially private filter works is that it

152
00:08:54,160 --> 00:08:59,910
looks for information that is consistent across multiple different individuals.

153
00:08:59,910 --> 00:09:02,880
It tries to filter out perfect differential privacy

154
00:09:02,880 --> 00:09:05,730
so no information leakage would, in theory,

155
00:09:05,730 --> 00:09:10,455
be able to block out any information that is unique about participants in

156
00:09:10,455 --> 00:09:12,460
your dataset and only let through

157
00:09:12,460 --> 00:09:15,630
information that is consistent across multiple different people,

158
00:09:15,630 --> 00:09:19,615
aka allowing you to learn what do tumors in humans look like

159
00:09:19,615 --> 00:09:22,600
without learning whether any individual person

160
00:09:22,600 --> 00:09:24,580
that you were studying actually had a tumor.

161
00:09:24,580 --> 00:09:29,225
That's the nature of this kind of a filter that differential privacy allows us to have.

162
00:09:29,225 --> 00:09:34,460
But it's only allowed to look for repeating statistical information inside the dataset.

163
00:09:34,460 --> 00:09:36,320
So if you have a really small data set,

164
00:09:36,320 --> 00:09:39,215
the odds of it finding the same statistical pattern twice

165
00:09:39,215 --> 00:09:42,880
are actually pretty low because you only have a few people to look at.

166
00:09:42,880 --> 00:09:45,010
If you have a dataset of five images,

167
00:09:45,010 --> 00:09:47,260
every image is totally unique,

168
00:09:47,260 --> 00:09:50,240
everything's going to look like it's totally private information,

169
00:09:50,240 --> 00:09:53,810
differential privacy will have to get rid of all the data,

170
00:09:53,810 --> 00:09:55,675
it won't let anything through,

171
00:09:55,675 --> 00:10:00,060
even a small amount of noise will totally corrupt the output of your query.

172
00:10:00,060 --> 00:10:04,000
However, if you have say a million images or maybe,

173
00:10:04,000 --> 00:10:06,615
in this case, we have 10,000 entries,

174
00:10:06,615 --> 00:10:08,660
then it becomes a lot easier to learn

175
00:10:08,660 --> 00:10:11,360
about general characteristics in the dataset because you have

176
00:10:11,360 --> 00:10:13,610
more data points to look at and compare with each other

177
00:10:13,610 --> 00:10:16,375
to look for a similar statistical information.

178
00:10:16,375 --> 00:10:18,870
So what are differentially private mechanisms?

179
00:10:18,870 --> 00:10:21,930
They're mechanisms that study a dataset and filter out

180
00:10:21,930 --> 00:10:26,240
any information that is unique to individual points

181
00:10:26,240 --> 00:10:29,930
in your dataset and try to let through information that is

182
00:10:29,930 --> 00:10:34,040
consistent across multiple different people in your dataset.

183
00:10:34,040 --> 00:10:38,620
This is why one of the big takeaways from this project in

184
00:10:38,620 --> 00:10:43,700
this lesson is really that the larger the corpus of information that you can work with,

185
00:10:43,700 --> 00:10:49,130
the easier it is for you to protect privacy because it's easier for your algorithm to

186
00:10:49,130 --> 00:10:55,095
detect that some statistical information is happening in more than one person,

187
00:10:55,095 --> 00:10:58,970
and thus is not private or unique or sensitive to that person

188
00:10:58,970 --> 00:11:03,340
because it's a general characteristic of humans more and more generally.

189
00:11:03,340 --> 00:11:05,750
So it gets a little bit of the philosophy differential privacy.

190
00:11:05,750 --> 00:11:09,610
In the next lesson, we'll really unpack what this means a bit more formally.

