1
00:00:00,000 --> 00:00:04,200
So in this project, we're going to use the pointer API that we've learned about in

2
00:00:04,200 --> 00:00:08,775
the last video and train a simple linear model on a remote worker.

3
00:00:08,775 --> 00:00:11,430
In this case, we're going to train it on Bob's worker.

4
00:00:11,430 --> 00:00:13,920
So first we want to create some training data.

5
00:00:13,920 --> 00:00:16,440
So let's create some toy input data.

6
00:00:16,440 --> 00:00:22,470
Now what I'd like to do here is initialized data that is about as small as possible.

7
00:00:22,470 --> 00:00:26,625
So this is not a real-world size neural network,

8
00:00:26,625 --> 00:00:29,100
but it's going to form the basis for what we

9
00:00:29,100 --> 00:00:31,485
do when learning some of the other techniques

10
00:00:31,485 --> 00:00:33,555
because federated learning and

11
00:00:33,555 --> 00:00:35,340
the different oxidation techniques and

12
00:00:35,340 --> 00:00:37,200
things that we're doing are complicated enough as it is.

13
00:00:37,200 --> 00:00:40,430
So we're going to work with a simple linear model for a lot

14
00:00:40,430 --> 00:00:43,760
of these exercises just to reduce the complexity of everything else was opposed

15
00:00:43,760 --> 00:00:47,450
to using like a big communist and lots of fancy features so that we can really focus

16
00:00:47,450 --> 00:00:51,560
just on the privacy preserving aspects of what we're trying to do.

17
00:00:51,560 --> 00:00:56,575
So we're going to say requires_grad=True, and we're going to send us to Bob.

18
00:00:56,575 --> 00:00:58,155
I'm going to create targets,

19
00:00:58,155 --> 00:01:00,545
and we're just going to learn a real linear relationship

20
00:01:00,545 --> 00:01:03,770
between this column being one and the target.

21
00:01:03,770 --> 00:01:07,140
So now Bob has a data set input target,

22
00:01:07,140 --> 00:01:08,515
both around Bob's machine.

23
00:01:08,515 --> 00:01:10,460
Now the next thing we need are just some linear weights,

24
00:01:10,460 --> 00:01:14,870
and create like this again requires_grad=True and send it to Bob.

25
00:01:14,870 --> 00:01:17,130
All right, so now Bob has our input data,

26
00:01:17,130 --> 00:01:19,145
has a target data, and has a set of weights.

27
00:01:19,145 --> 00:01:22,160
So let's start by just doing a little bit of forward propagation.

28
00:01:22,160 --> 00:01:26,440
So our prediction will just be input matrix multiplied by the weights,

29
00:01:26,440 --> 00:01:30,095
prediction is also a pointer as you might expect the loss

30
00:01:30,095 --> 00:01:33,650
is pred minus target squared to two in the sum.

31
00:01:33,650 --> 00:01:36,755
So this is mean squared error loss. Oops, typo there.

32
00:01:36,755 --> 00:01:39,475
Then we backpropagate, and then we do a weight update.

33
00:01:39,475 --> 00:01:41,580
This is our alpha of 0.1,

34
00:01:41,580 --> 00:01:43,700
then we want to zero out our gradients when we're done.

35
00:01:43,700 --> 00:01:47,120
So that small typo and was your grades at

36
00:01:47,120 --> 00:01:51,050
the end so they don't accumulate over time and then print loss.

37
00:01:51,050 --> 00:01:54,835
Of course we have to actually get the loss so we can see it.

38
00:01:54,835 --> 00:02:00,340
Beautiful. If we do this several times hopefully,

39
00:02:00,340 --> 00:02:02,000
we should see the loss start to go down.

40
00:02:02,000 --> 00:02:04,485
Look at that. Let's do an iterator.

41
00:02:04,485 --> 00:02:05,945
So as you can see,

42
00:02:05,945 --> 00:02:09,520
once we have moved our data and our weights to row machine,

43
00:02:09,520 --> 00:02:15,490
all this code is just using native normal run of the mill PyTorch.

44
00:02:15,490 --> 00:02:19,270
The idea is that for this tool to make it so that things like federated learning and

45
00:02:19,270 --> 00:02:23,695
remote execution is dead simple if you already know how to use PyTorch.

46
00:02:23,695 --> 00:02:25,400
Now, in the next section,

47
00:02:25,400 --> 00:02:27,880
we're going to explore a couple of the "gotchas" and

48
00:02:27,880 --> 00:02:31,440
other things that can be a bit more challenging to sit or understand,

49
00:02:31,440 --> 00:02:33,580
or perhaps debug when you're on

50
00:02:33,580 --> 00:02:38,035
a remote machine or when you're when you have a connection to a remote machine.

51
00:02:38,035 --> 00:02:40,410
So the next lesson, I'll see you there.

