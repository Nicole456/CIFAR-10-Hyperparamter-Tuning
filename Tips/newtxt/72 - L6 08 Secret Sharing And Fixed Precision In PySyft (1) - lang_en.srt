So in this video, we're going to take some of
the theoretical concepts we've been learning about in the last few sections,
specifically additive secret sharing and fixed precision
representation and show how you can leverage PySyft to do
this in sort of an easy and intuitive way where it just feels like you're
using normal PyTorch tensors
even though they're being encoded in this way under the hood.
But the first thing I'd like for us to do is clear out
the objects that are inside of Bob,
Alice, and secure worker,
we're going to use these as our example workers to share things on,
and then we want to create a little bit of data.
So do our standard 1, 2, 3, 4,
5 tensor. Looks good,
and now let's show you this next method called share.
Now, we can pass in a series of workers here,
arbitrary number, either 2 or 3 or 20,
and what this will do is actually split this x value into
multiple different additive secret shares and then send those shares to Bob,
Alice, and secure worker such that we will have pointers to that data.
So let me show you what I mean.
So when we call.share,
what it did on the hood has generated
three different pointers to the three different shares that we had sent to Bob,
Alice, and secure worker.
So now, if we actually take a look at say one of these shares,
we can see is a bunch of large random numbers.
However, the nice thing about this is that we can actually,
again just like we did with regular pointer tensors,
just pretend that there's another tensor.
As you can see, it's created another share on Bob's machine,
created another share on all three of these guys, so Bob,
Alice, and secure worker,
and y is another additively shared tensor.
If we call y.get,
it will decrypt this encrypted tensor and gives us back the result of our computation,
the result of our addition 2,
4, 6, 8, 10.
Now, however, we did mention before that these are integers,
so we're doing integer computation here.
Whereas in the context of federate learning,
we want to be able to do this on decimal valued numbers.
So this is where a PySyft also adds in a bit of extra functionality.
So let's say we have our canonical tensor again,
but this time we're going to say actually now we'll suit decimal values.
So 0.1, 0.2, 0.3, 0.4, 0.5.
Cool. Now, we're going to encode
this using fixed precision and x is now our fixed precision number.
Under the hood inside of x are the values 100,
200, 300, 400, and 500.
However, this is still getting interpreted in
all the mathematical operations are such that if
we were to go back to a regular encoding,
it would still look like this 0.1, 0.2, 0.3, 0.4.
So if I go to x.float_precision,
excuse me, so you can see it restores the original values.
Now, you might find this to be an interesting representation.
This actually alludes to a little bit of how PySyft works under the hood.
So this is actually a tensor chain.
At the top, we have sort of a dummy PyTorch wrapper.
There are reasons why we have to do this to make it
compatible with rest of the PyTorch ecosystem.
This forwards all commands to it's child, so basically x.child.
So let me just show you this.
I'll create another fixed precision tensor.
So the type of x is a Torch tensor.
The type of x.child is a fixed precision tensor.
The type of x.child.child, which you can see where this is going,
is another native tensor.
So this native tensor is actually the raw data.
This is the actual encoding of this 100,
200, 300, 400, and 500.
This is what we call an interpreter.
So whenever you execute a command,
it gets called on this wrapper on the top which
then passes it off to.child automatically.
It will always pass it off to.child..child says, "Okay.
You're trying to say add two tensors together."
Well, since I am a fixed precision interpreter,
I know how to add this appropriately given
the fact that the underlying data is encoded in a particular way.
This is how it knows how to do
the appropriate operations because on this sort of specially encoded data,
which it then calls on.child which actually manipulates the data.
So as it turns out, pointers actually work this way too. All right.
So pointer is a special type that when the wrapper calls the pointer,
the pointer knows how to handle the remote encoding of this particular data.
So it's just a little bit on how PySyft tensors work in
a bit more advanced way and what this print statement is really saying.
It's sort of saying we have wrapper and this little
caret says it's sort of a.child signal.
So this is the child of the wrapper and in
this actual data is a child of the fixed precision tensor.
So there you go. Of course,
we can go y equals x plus x,
y is still a fixed precision tensor, y equals y.float_precision.
There you go, we have the result of our computation.
As you might expect,
we can actually use these together.
So if I said x equals th.tensor([0.1, 0.2,
0.3]) and I said fixed precision and share with Bob, Alice,
and secure worker, y equals x,
so as you can see, this is the nesting.
So at the top we have the wrapper.
This goes to a fixed precision tensor which says, "Okay.
Interpret everything beneath me as if it's actually a decimal,
but I know that it's going to be encoded as an integer."
It's child is an additive sharing tensor which knows how to encode integers.
So as you remember here,
the child of a fixed precision tensor
always has to be an integer tensor because the whole point is to be able to
store a theoretically decimal value number as an integer under the hood.
Since additive secret sharing knows how to deal with integers,
it then has these pointers to Bob,
Alice, and secure worker.
So I can go y equals x plus x. Y also ends up being
a fixed precision tensor wrapping and additive sharing tensor and I can
call y equals y.get.float_precision,
and now we have the result of our computation.
So I hope you really see the power that the PySyft is able to give you,
these multiple layers of interpretation,
of abstraction, and of message forwarding to remote machines.
In the next section,
we're going to talk about the next project that you want to be able to
use these kinds of tools for. So I'll see you then.
